{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7690995,
          "sourceType": "datasetVersion",
          "datasetId": 4488372
        },
        {
          "sourceId": 10207098,
          "sourceType": "datasetVersion",
          "datasetId": 6308032
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puEkKyADAOoY",
        "outputId": "cd09d5a3-5d9a-40de-ee7a-1df5b028e68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:34:41.512047Z",
          "iopub.execute_input": "2024-12-15T15:34:41.512460Z",
          "iopub.status.idle": "2024-12-15T15:34:41.519786Z",
          "shell.execute_reply.started": "2024-12-15T15:34:41.512431Z",
          "shell.execute_reply": "2024-12-15T15:34:41.518892Z"
        },
        "id": "lJ7EYRg0_Whs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:34:41.580200Z",
          "iopub.execute_input": "2024-12-15T15:34:41.580901Z",
          "iopub.status.idle": "2024-12-15T15:34:49.747489Z",
          "shell.execute_reply.started": "2024-12-15T15:34:41.580864Z",
          "shell.execute_reply": "2024-12-15T15:34:49.746366Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRXQyl8x_Whw",
        "outputId": "4754f005-9917-46e1-9f6c-3e3b0156fb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Modified for Kaggle: Batch Job Recommendations & Total Experience Calculation\n",
        "\n",
        "# Install necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import os\n",
        "from datetime import datetime\n",
        "from PyPDF2 import PdfReader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:34:49.749637Z",
          "iopub.execute_input": "2024-12-15T15:34:49.749929Z",
          "iopub.status.idle": "2024-12-15T15:34:50.360412Z",
          "shell.execute_reply.started": "2024-12-15T15:34:49.749903Z",
          "shell.execute_reply": "2024-12-15T15:34:50.359467Z"
        },
        "id": "7TTI2ChJ_Whw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the pickle file exists\n",
        "if os.path.exists(\"job_df.pkl\"):\n",
        "    job_df = pd.read_pickle(\"job_df.pkl\")\n",
        "    print(\"DataFrame loaded from pickle file.\")\n",
        "else:\n",
        "    # Load the dataset from Kaggle's \"Input\" section\n",
        "  #  csv_file_path = \"/kaggle/input/upwork-job-postings-dataset-2024-50k-records/upwork-jobs.csv\"  # Replace with your dataset path\n",
        "    job_df = pd.read_csv(\"./input/upwork-jobs.csv\")  # Replace with your dataset path\n",
        "    job_df.to_pickle(\"job_df.pkl\")\n",
        "    print(\"DataFrame saved successfully as job_df.pkl.\")\n",
        "\n",
        "\n",
        "# Step 1: Load the Resume\n",
        "\n",
        "def load_resume(file_path):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PdfReader(file)\n",
        "            text = \"\".join([page.extract_text() for page in reader.pages ])\n",
        "    elif file_path.endswith('.docx'):\n",
        "        import docx2txt\n",
        "        text = docx2txt.process(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")\n",
        "    return text\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:34:50.362077Z",
          "iopub.execute_input": "2024-12-15T15:34:50.362461Z",
          "iopub.status.idle": "2024-12-15T15:34:50.531984Z",
          "shell.execute_reply.started": "2024-12-15T15:34:50.362422Z",
          "shell.execute_reply": "2024-12-15T15:34:50.531088Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yidHArF2_Whx",
        "outputId": "70c6906d-0b99-4b41-f1fa-8cec4a3e7acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame loaded from pickle file.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install --upgrade pydantic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdHDWfMdHDpK",
        "outputId": "f3a34cd3-8b10-45c3-bb57-16dcf0a09a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
            "Collecting openai\n",
            "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed openai-1.57.4\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ENH7mrtaHDsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Vectorization and Similarity Calculation (Batch PRocessing)\n",
        "\n",
        "# def calculate_similarity(resume_text, job_descriptions):\n",
        "#     vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
        "#     combined_texts = [resume_text] + job_descriptions\n",
        "#     tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
        "#     similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "#     return similarity_matrix[0, 1:]\n",
        "\n",
        "\n",
        "def calculate_similarity_in_batches(resume_text, job_descriptions, batch_size=5000):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "    similarities = []\n",
        "\n",
        "    for i in range(0, len(job_descriptions), batch_size):\n",
        "        batch = job_descriptions[i:i + batch_size]\n",
        "        combined_texts = [resume_text] + batch\n",
        "        tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
        "        batch_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
        "        similarities.extend(batch_similarities)\n",
        "\n",
        "    return similarities"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:35:01.370581Z",
          "iopub.execute_input": "2024-12-15T15:35:01.370794Z",
          "iopub.status.idle": "2024-12-15T15:35:01.381698Z",
          "shell.execute_reply.started": "2024-12-15T15:35:01.370772Z",
          "shell.execute_reply": "2024-12-15T15:35:01.380970Z"
        },
        "id": "tdqsGfFL_Why"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Match and Rank Jobs\n",
        "\n",
        "# def match_jobs(resume_text, job_db):\n",
        "#     job_db_subset = job_db.copy()\n",
        "#     similarities = calculate_similarity(resume_text, job_db_subset['description'].tolist())\n",
        "#     job_db_subset['similarity_score'] = similarities  # No normalization\n",
        "#     sorted_jobs = job_db_subset.sort_values(by='similarity_score', ascending=False)\n",
        "#     return sorted_jobs\n",
        "\n",
        "def match_jobs(resume_text, job_db_path, chunksize=5000):\n",
        "    top_matches = []\n",
        "    for chunk in pd.read_csv(job_db_path, chunksize=chunksize):\n",
        "        similarities = calculate_similarity_in_batches(resume_text, chunk['Resume'].tolist())\n",
        "        chunk['similarity_score'] = similarities\n",
        "        top_matches.append(chunk.sort_values(by='similarity_score', ascending=False).head(10))\n",
        "    return pd.concat(top_matches).sort_values(by='similarity_score', ascending=False).head(10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:35:01.382685Z",
          "iopub.execute_input": "2024-12-15T15:35:01.382936Z",
          "iopub.status.idle": "2024-12-15T15:35:01.396734Z",
          "shell.execute_reply.started": "2024-12-15T15:35:01.382895Z",
          "shell.execute_reply": "2024-12-15T15:35:01.395982Z"
        },
        "id": "_c6VU4xn_Whz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_information_from_resume(resume_text):\n",
        "    \"\"\"\n",
        "    Extract structured information from resume text.\n",
        "\n",
        "    Parameters:\n",
        "        resume_text (str): The input text of the resume.\n",
        "\n",
        "    Returns:\n",
        "        dict: Extracted information including Skills, Education, Work Experience, and Total Years of Experience.\n",
        "    \"\"\"\n",
        "    # Initialize dictionary to store extracted info\n",
        "    extracted_info = {\n",
        "        \"Skills\": [],\n",
        "        \"Education\": [],\n",
        "        \"Work Experience\": [],\n",
        "        \"Total Years of Experience\": \"\"\n",
        "    }\n",
        "\n",
        "    # Extract Skills\n",
        "    skills_match = re.findall(r\"Skills?:\\s*(.*?)\\n\", resume_text, re.IGNORECASE | re.DOTALL)\n",
        "    if skills_match:\n",
        "        extracted_info[\"Skills\"] = [skill.strip() for skill in skills_match[0].split(',')]\n",
        "\n",
        "    # Extract Education or Qualifications\n",
        "    education_match = re.findall(r\"Education|Qualifications:?\\s*(.*?)\\n\", resume_text, re.IGNORECASE)\n",
        "    if education_match:\n",
        "        extracted_info[\"Education\"] = education_match\n",
        "\n",
        "    # Extract Work Experience\n",
        "    work_experience_match = re.findall(r\"(Work Experience|Experience):?\\s*(.*?)(?:\\n\\n|\\Z)\", resume_text, re.IGNORECASE | re.DOTALL)\n",
        "    if work_experience_match:\n",
        "        work_experience_details = work_experience_match[0][1]\n",
        "        extracted_info[\"Work Experience\"] = [exp.strip() for exp in work_experience_details.split('\\n') if exp.strip()]\n",
        "\n",
        "    # Extract Total Years of Experience\n",
        "    years_match = re.findall(r\"(\\d+)\\s*(years?|yrs?)\\s*experience\", resume_text, re.IGNORECASE)\n",
        "    if years_match:\n",
        "        extracted_info[\"Total Years of Experience\"] = f\"{years_match[0][0]} years\"\n",
        "\n",
        "    # Return the structured information\n",
        "    return extracted_info\n",
        "\n",
        "\n",
        "# Example usage\n",
        "resume_text = \"\"\"\n",
        "John Doe\n",
        "Skills: Python, Data Analysis, SQL, Machine Learning\n",
        "Education: B.Sc. in Computer Science, XYZ University, 2020\n",
        "Work Experience:\n",
        "1. Data Scientist at ABC Corp (2021-2023)\n",
        "2. Software Engineer at DEF Inc (2019-2020)\n",
        "Total Years of Experience: 4 years\n",
        "\"\"\"\n",
        "\n",
        "extracted_info = extract_information_from_resume(resume_text)\n",
        "\n",
        "print(\"Extracted Information:\")\n",
        "for key, value in extracted_info.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ8RtElaJFnq",
        "outputId": "8d8f255e-cc18-4c2b-de50-0ab2636f403a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Information:\n",
            "Skills: ['Python', 'Data Analysis', 'SQL', 'Machine Learning']\n",
            "Education: ['']\n",
            "Work Experience: ['1. Data Scientist at ABC Corp (2021-2023)', '2. Software Engineer at DEF Inc (2019-2020)', 'Total Years of Experience: 4 years']\n",
            "Total Years of Experience: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the resume\n",
        "    resume_file_path = \"./kaggle/input/resume.pdf\"  # Replace with your Kaggle path\n",
        "    if os.path.exists(resume_file_path):\n",
        "        resume_text = load_resume(resume_file_path)\n",
        "        print(resume_text)\n",
        "\n",
        "        # Extract information from resume\n",
        "        resume_info = extract_information_from_resume(resume_text)\n",
        "        # display_extracted_info(resume_info)\n",
        "        print(extracted_info)\n",
        "\n",
        "        # Load job dataset\n",
        "        job_dataset_path = \"./input/Updated_Job_Dataset.csv\"  # Replace with your dataset path\n",
        "        if os.path.exists(job_dataset_path):\n",
        "            job_df = pd.read_csv(job_dataset_path)\n",
        "\n",
        "            # Match jobs and get recommendations\n",
        "            recommended_jobs = match_jobs(resume_text, job_dataset_path)\n",
        "\n",
        "            print(\"\\nTop 10 Recommended Jobs:\")\n",
        "            #print(recommended_jobs[['title', 'description', 'published_date', 'country', 'similarity_score']].head(10))\n",
        "            print(recommended_jobs[['Category', 'similarity_score']].head(10))\n",
        "        else:\n",
        "            print(f\"Error: Job dataset file '{job_dataset_path}' not found.\")\n",
        "    else:\n",
        "        print(f\"Error: Resume file '{resume_file_path}' not found.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-15T15:38:03.758160Z",
          "iopub.execute_input": "2024-12-15T15:38:03.758954Z",
          "iopub.status.idle": "2024-12-15T15:38:14.097000Z",
          "shell.execute_reply.started": "2024-12-15T15:38:03.758922Z",
          "shell.execute_reply": "2024-12-15T15:38:14.096052Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15E108Zd_Whz",
        "outputId": "7defa5ee-90c4-4786-c997-979f1ae7b06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Govind Sunilkumar\n",
            "Aspiring Digital Electronics Engineer\n",
            "♂phone+1-(945)2808460 /envel⌢pegovindpsunilkumar@gmail.com /linkedinGovind-Sunilkumar\n",
            "Education\n",
            "Masters of Science in Computer Engineering Aug 2023 - Dec 2024\n",
            "Virginia Tech, Blacksburg CGPA: 4.0\n",
            "B.Tech in Electronics and Communication Engineering Sep 2017 - May 2021\n",
            "National Insititute of Technology, Karnataka CGPA: 7.35/10\n",
            "Professional Experience\n",
            "RTL Systems Design Intern, Skyworks Solutions, Austin May 2024 - Aug 2024\n",
            "•Designed Skyworks proprietary communication protocol test driver on Zynq ZCU 102 FPGA , used Vivado flow to\n",
            "generate bit stream and VBScript to test driver on PetaLinux Environment\n",
            "•Executed Silicon Post-Validation tests on prototype test chip for Skyworks Solutions\n",
            "Embedded Hardware Engineer, Bharat Electronics Limited, Bangalore Oct 2021 - Jun 2023\n",
            "•Designed and Tested Microchip PIC microcontroller based Display System for aerospace applications using OrCAD\n",
            "•Tested with I2C interface and developed Embedded C code that would communicate with board\n",
            "•Designed and Tested 24v down-converted circuit board for laser positioning system\n",
            "Projects\n",
            "Design 12 bit Multiplier logic and layout Nov 2023\n",
            "•Designed a novel 12x12 Vedic multiplier using stackable carry look-ahead adders for decreased area and delay.\n",
            "•Utilized Cadence Virtuoso 45nm PDK for schematic design and layout.\n",
            "•Estimated performance increase using ADEP metrics\n",
            "•Achieved a 25% reduction from commonly used booth multiplier for unsigned multiplication\n",
            "Technical Skills\n",
            "Programming : System Verilog, VHDL, Visual Basic, Scala, Embedded C, Python, C++, Scala.\n",
            "Software : OrCAD, Allegro, Vivado, Cadence Virtuoso.\n",
            "Skills : Digital Design, Post Silicon Validation, Board Schematic Design and Testing, Physical Design and layout, RISC V\n",
            "Architecture.\n",
            "Relevant Coursework\n",
            "•Building a RISC V Core\n",
            "using TL-Verilog (EdX)\n",
            "•Digital Design•Modern Binary\n",
            "Exploitation\n",
            "•Embedded RTOS•Computer Architecture\n",
            "•Digital Systems testing\n",
            "and verification•VLSI IC Design\n",
            "•Embedded with RISC V\n",
            "(Udemy)\n",
            "Academics Accomplishments & Extracurricular\n",
            "•Graduate Teaching Assistant at Virginia Tech, Subject : Applied Electrical Theory\n",
            "•Received best Evaluator and best role taker award, NITK Toastmasters Club meeting, Oct 2019\n",
            "•Participated in the Annual Speech contest of Toastmasters International , Mar 2020\n",
            "•Served as the Content Head and executive member of HackVerse Hackathon Organising team , NITK 2020–21\n",
            "•Served as the Robotics head of the Institution of Engineers,NITK 2020–21\n",
            "{'Skills': ['Python', 'Data Analysis', 'SQL', 'Machine Learning'], 'Education': [''], 'Work Experience': ['1. Data Scientist at ABC Corp (2021-2023)', '2. Software Engineer at DEF Inc (2019-2020)', 'Total Years of Experience: 4 years'], 'Total Years of Experience': ''}\n",
            "\n",
            "Top 10 Recommended Jobs:\n",
            "                             Category  \\\n",
            "525     Sensor Integration Specialist   \n",
            "534                     MEMS Engineer   \n",
            "452            Analog Design Engineer   \n",
            "441                 Firmware Engineer   \n",
            "582  Wireless Communications Engineer   \n",
            "424              FPGA Design Engineer   \n",
            "402  Wireless Communications Engineer   \n",
            "28               Full Stack Developer   \n",
            "459        Embedded Software Engineer   \n",
            "497     Sensor Integration Specialist   \n",
            "\n",
            "                                                Resume  similarity_score  \n",
            "525  Proficient in Verilog, VHDL, and RTL design fo...          0.112264  \n",
            "534  Proficient in Verilog, VHDL, and RTL design fo...          0.112264  \n",
            "452  Proficient in Verilog, VHDL, and RTL design fo...          0.112264  \n",
            "441  Proficient in Verilog, VHDL, and RTL design fo...          0.112264  \n",
            "582  Proficient in Verilog, VHDL, and RTL design fo...          0.112264  \n",
            "424  Proficient in Verilog, VHDL, and RTL design fo...          0.112264  \n",
            "402  Proficient in Verilog, VHDL, and RTL design fo...          0.112264  \n",
            "28   Motivated Full Stack Developer with a strong f...          0.079397  \n",
            "459  Skilled in FPGA prototyping and hardware verif...          0.078267  \n",
            "497  Skilled in FPGA prototyping and hardware verif...          0.078267  \n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}